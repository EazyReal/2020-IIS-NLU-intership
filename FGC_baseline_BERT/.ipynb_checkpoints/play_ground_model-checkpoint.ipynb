{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader, Model, Trainning\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0706 08:57:47.469936 139908435564352 file_utils.py:39] PyTorch version 1.5.1 available.\n",
      "I0706 08:57:49.104424 139908435564352 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Path ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './FGC_release_1.7.13/FGC_release_all_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default tolkenizer is used\n",
    "train_set = FGC_Dataset(train_data_path, \"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '苏', '东', '坡', '在', '中', '国', '历', '史', '上', '，', '是', '哪', '一', '个', '朝', '代', '的', '人', '？', '[SEP]', '苏', '轼', '（', '103', '##7', '年', '1', '月', '8', '日', '－', '110', '##1', '年', '8', '月', '24', '日', '）', '，', '[SEP]'] \n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) \n",
      " tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.tokenizer.convert_ids_to_tokens(train_set[0][0]), \"\\n\",\n",
    "train_set[0][1],\"\\n\",\n",
    "train_set[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataLoader for minibatch\n",
    "in each batch\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # use(have) label or not\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad to same length\n",
    "    tokens_tensors = pad_sequence(tokens_tensors,  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,  batch_first=True)\n",
    "    \n",
    "    # attention masks, set none-padding part to 1 for LM to attend\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill( tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "# collate_fn : list of instances to minibatch\n",
    "trainloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    tokens_tensors.shape   = torch.Size([64, 55]) \n",
      "    tensor([[ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0]])\n",
      "    ------------------------\n",
      "    segments_tensors.shape = torch.Size([64, 76])\n",
      "    tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "    ------------------------\n",
      "    masks_tensors.shape    = torch.Size([64, 55])\n",
      "    tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "    ------------------------\n",
      "    label_ids.shape        = torch.Size([64])\n",
      "    tensor([ True, False,  True, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    data = next(iter(trainloader))\n",
    "\n",
    "    tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "    print(f\"\"\"\n",
    "    tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "    {tokens_tensors}\n",
    "    ------------------------\n",
    "    segments_tensors.shape = {segments_tensors.shape}\n",
    "    {segments_tensors}\n",
    "    ------------------------\n",
    "    masks_tensors.shape    = {masks_tensors.shape}\n",
    "    {masks_tensors}\n",
    "    ------------------------\n",
    "    label_ids.shape        = {label_ids.shape}\n",
    "    {label_ids}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
