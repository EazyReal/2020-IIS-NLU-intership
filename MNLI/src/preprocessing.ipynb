{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self\n",
    "import config\n",
    "from preprocess import *\n",
    "from utils import *\n",
    "\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import math # for sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/2020-IIS-NLU-internship/MNLI/data/multinli_1.0/multinli_1.0_dev_matched.jsonl\n"
     ]
    }
   ],
   "source": [
    "dev_path = config.DEV_MA_FILE\n",
    "print(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.BERT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9815\n"
     ]
    }
   ],
   "source": [
    "dev_set = MNLI_Raw_Dataset(dev_path, mode=\"develop\")\n",
    "print(len(dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to create batch for graph???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': {'input_ids': tensor([[  101,  5898, 16283,  2015,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  3930,  1004, 23713,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2748,  1010, 17319,  1996,  2192,  1997,  2720,  1012,  2829,\n",
      "           999,  2720,  1012,  5708,  5864,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 11463, 22436,  2378,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2008,  1005,  1055,  2995,  8529,  1011, 14910,  2092,  2008,\n",
      "          1005,  1055,  2995,  1996,  2637,  1005,  1055,  7079,  2035,  2023,\n",
      "          2769,  2000,  2031,  2060,  2111,  2507,  7910,  4681,  2000,  2060,\n",
      "          3032,  2061,  2027,  2071,  2022,  7079,  2037,  2219,  2111,  1998,\n",
      "          2731,  2037,  2219,  2111,  2012,  1996,  2168,  2051,   102,     0,\n",
      "             0],\n",
      "        [  101,  2197,  2095,  1010,  2027,  2020, 11867, 14659,  2098,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2156,  1996,  2801,  1029,  1000,  3486,  1000,  2059,  2017,\n",
      "          2228,  1000, 10722, 21512,  5897,  5864,  2000, 10616,  1996, 10514,\n",
      "          9397, 19234,  3929,  1000,  2008,  2009,  2001,  2004,  4869,  9303,\n",
      "          2008,  2027,  2359,  2033,  2000,  2175,  2000,  3000,  1029,  1000,\n",
      "          2720,  1012,  5708,  3281,  2062,  4929,  6588,  2084,  2412,  1012,\n",
      "           102],\n",
      "        [  101,  8529,  1011, 14910,  3398,  1045,  2113,  2054,  2008,  1005,\n",
      "          1055,  2066,  7910,  1011,  9616,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]])}, 'sentence2': {'input_ids': tensor([[  101,  1996, 16283,  2015,  2024,  2081,  1997,  6557,  1998, 20392,\n",
      "          8313,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4935,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2045,  2003,  2053,  4797,  2008,  2192,  7460,  2000,  2720,\n",
      "          1012,  2829,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2009,  3397, 11463, 22436,  2378,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2637,  2038,  2985,  2498,  2006,  3097,  4681,  2061,  2129,\n",
      "          2052,  2027,  2022,  2583,  2000,  3345,  1998,  3477,  2037,  2219,\n",
      "          2111,  1012,   102],\n",
      "        [  101,  2027,  2020, 11867, 14659,  2100,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 10722, 21512,  5897,  2074,  2481,  1005,  1056, 10616,  1996,\n",
      "          4145,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1045,  2031,  2053,  2801,  2054,  2008,  2003,  2066,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}, 'gold_label': tensor([0, 2, 2, 1, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "dev_loader = DataLoader(\n",
    "    dataset=dev_set, \n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,  \n",
    "    collate_fn=create_mini_batch\n",
    ")\n",
    "if config.DEBUG:\n",
    "    print(next(iter(dev_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] nobody expects that the devil would take the form of a lawyer . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "if config.DEBUG:\n",
    "    print(tensor_to_sent(next(iter(dev_loader))[config.h_field]['input_ids'][0], tokenizer))\n",
    "    print(type(next(iter(dev_loader))[config.h_field]['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossBERTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    bert cross attention model\n",
    "    h, p go through bert and get their contexulized embedding saparately\n",
    "    and do soft alignment and prediction as in decomp-att paper\n",
    "    this is a embedding enhanced version of decomp-att\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_encoder=None, cross_attention_hidden=392):\n",
    "        super().__init__()\n",
    "        #bert encoder\n",
    "        if bert_encoder == None or not isinstance(bert_encoder, BertModel):\n",
    "            print(\"unkown bert model choice, init with config.BERT_EMBEDDING\")\n",
    "            bert_encoder = BertModel.from_pretrained(config.BERT_EMBEDDING)\n",
    "        self.bert_encoder = bert_encoder\n",
    "        # dropouts\n",
    "        self.dropout = nn.Dropout(p=bert_encoder.config.hidden_dropout_prob)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        # linear layers for cross attention, with biased?\n",
    "        self.cross_attention_hidden = cross_attention_hidden\n",
    "        self.Wq = nn.Parameter(torch.Tensor(bert_encoder.config.hidden_size, self.cross_attention_hidden))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(bert_encoder.config.hidden_size, self.cross_attention_hidden))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(bert_encoder.config.hidden_size, bert_encoder.config.hidden_size))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(bert_encoder.config.hidden_size, bert_encoder.config.hidden_size))\n",
    "        self.classifier = nn.Linear(bert_encoder.config.hidden_size, 1)\n",
    "        \n",
    "        forward_expansion = 1 # can change\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(bert_encoder.config.hidden_size, forward_expansion*bert_encoder.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*bert_encoder.config.hidden_size, bert_encoder.config.hidden_size),\n",
    "        )\n",
    "        # critrion add positive weight\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \"\"\"\n",
    "    cross attention, similar to Decomp-Att\n",
    "    but no fowrad nn, use Wk Wq Wv\n",
    "    input: query vector(b*n*d), content vector(b*m*d)\n",
    "    ouput: sof aligned content vector to query vector(b*n*d)\n",
    "    \"\"\"\n",
    "    def cross_attention(self, h1, h2):\n",
    "        Q = torch.matmul(h1, self.Wq)\n",
    "        K = torch.matmul(h2, self.Wk)\n",
    "        V = torch.matmul(h2, self.Wv)\n",
    "        #Kt = torch.matmul(h2, self.Wk).permute(0,2,1)\n",
    "        #E = torch.matmul(Q, Kt)\n",
    "        E = torch.einsum(\"bnd,bmd->bnm\", [Q, K]) # batch, n/m, dimension\n",
    "        A = torch.softmax(E / (math.sqrt(self.cross_attention_hidden)), dim=2) #soft max dim = 2\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        aligned_2_for_1 = torch.einsum(\"bnm,bmd->bnd\", [A, V])\n",
    "        if(config.DEBUG):\n",
    "            print(Q.size())\n",
    "            print(K.size())\n",
    "            print(E.size())\n",
    "            print(A.size())\n",
    "            print(aligned_2_for_1.size())\n",
    "            \n",
    "        return aligned_2_for_1\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        'sentence1' : {'input_ids', 'token_type_ids', 'attention_mask'}\n",
    "        'sentence2'\n",
    "        'gold_label'\n",
    "        \"\"\"\n",
    "        # get bert contextualized embedding\n",
    "        # the _ here is the last hidden states\n",
    "        # q_poolout is a 768-d vector of [CLS]\n",
    "        # q_poolout = self.dropout(q_poolout), MT Wu : no dropout better, without \n",
    "        hidden = [None, None]\n",
    "        q_poolout = [None, None]\n",
    "        hidden[0], q_poolout[0] = self.bert_encoder(input_ids=batch[config.h_field]['input_ids'],\n",
    "                                         token_type_ids=batch[config.h_field]['token_type_ids'],\n",
    "                                         attention_mask=batch[config.h_field]['attention_mask'])\n",
    "        hidden[1], q_poolout[1] = self.bert_encoder(input_ids=batch[config.p_field]['input_ids'],\n",
    "                                         token_type_ids=batch[config.p_field]['token_type_ids'],\n",
    "                                         attention_mask=batch[config.p_field]['attention_mask'])\n",
    "        # soft alignment\n",
    "        aligned_p_for_h = self.cross_attention(hidden[0], hidden[1])\n",
    "        aligned_h_for_p = self.cross_attention(hidden[1], hidden[0])\n",
    "        if(config.DEBUG):\n",
    "            print(\"hidden[0] (hypothesis) size : \" + str(hidden[0].size()))\n",
    "            print(\"aligned_p_for_h size : \" + str(aligned_p_for_h.size()))\n",
    "        logits = self.classifier(q_poolout)\n",
    "        # can apply nn.module.Sigmoid here to convert to p-distribution\n",
    "        # score is indeed better (and more stable)\n",
    "        logits = logits.squeeze(-1)\n",
    "        return logits\n",
    "    \n",
    "    # the nn.Module method\n",
    "    def forward_2(self, batch):\n",
    "        logits = self.forward_nn(batch)\n",
    "        batch[3] = batch[3].to(dtype=torch.float)\n",
    "        loss = self.criterion(logits, batch[3])\n",
    "        return loss\n",
    "    \n",
    "    # return sigmoded score\n",
    "    def _predict_score(self, batch):\n",
    "        logits = self.forward_nn(batch)\n",
    "        scores = torch.sigmoid(logits)\n",
    "        scores = scores.detach().cpu().numpy().tolist()\n",
    "        return scores\n",
    "    \n",
    "    # return True False based on score + threshold\n",
    "    def _predict(self, batch, threshold=0.5):\n",
    "        scores = self._predict_score(batch)\n",
    "        return [ 1 if score >= threshold else 0 for score in scores]\n",
    "    \n",
    "    # return result with assigned threshold, default = 0.5\n",
    "    def predict_fgc(self, q_batch, threshold=0.5):\n",
    "        scores = self._predict(q_batch)\n",
    "\n",
    "        max_i = 0\n",
    "        max_score = 0\n",
    "        sp = []\n",
    "        for i, score in enumerate(scores):\n",
    "            if score > max_score:\n",
    "                max_i = i\n",
    "                max_score = score\n",
    "            if score >= threshold:\n",
    "                sp.append(i)\n",
    "\n",
    "        if not sp:\n",
    "            sp.append(max_i)\n",
    "\n",
    "        return {'sp': sp, 'sp_scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unkown bert model choice, init with config.BERT_EMBEDDING\n"
     ]
    }
   ],
   "source": [
    "model = CrossBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "18\n",
      "8\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(test_batch[config.h_field][\"input_ids\"]))\n",
    "print(len(test_batch[config.h_field][\"input_ids\"][5]))\n",
    "print(len(test_batch[config.p_field][\"input_ids\"]))\n",
    "print(len(test_batch[config.p_field][\"input_ids\"][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18, 392])\n",
      "torch.Size([8, 54, 392])\n",
      "torch.Size([8, 18, 54])\n",
      "torch.Size([8, 18, 54])\n",
      "torch.Size([8, 18, 768])\n",
      "hidden[0] (hypothesis) size : torch.Size([8, 18, 768])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'aligned_h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-f1d61db6563d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-42d878d875d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden[0] (hypothesis) size : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aligned_p_for_h size : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_poolout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# can apply nn.module.Sigmoid here to convert to p-distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aligned_h' is not defined"
     ]
    }
   ],
   "source": [
    "model(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
